{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccf74a3a-951a-4781-95a3-1513dc37d85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to configure Gemini API Key with google.genai SDK...\n",
      "Gemini API Client initialized successfully.\n",
      "\n",
      "Available models (some might be specific to this client/region):\n",
      "\n",
      "Attempting to generate content...\n",
      "An AttributeError occurred: 'Client' object has no attribute 'generate_content'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass # For prompting if needed\n",
    "# This import is characteristic of the newer google-genai SDK\n",
    "from google import genai\n",
    "\n",
    "print(\"Attempting to configure Gemini API Key with google.genai SDK...\")\n",
    "\n",
    "try:\n",
    "    # Try to get the API key from the correct environment variable\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\") # Changed to GEMINI_API_KEY\n",
    "\n",
    "    if not api_key:\n",
    "        print(\"GEMINI_API_KEY environment variable not found.\")\n",
    "        print(\"Attempting to prompt for API key...\")\n",
    "        api_key = getpass.getpass('Enter your Gemini API Key: ')\n",
    "\n",
    "    if api_key:\n",
    "        # Initialize the client with the API key\n",
    "        # This is the key change for the newer SDK\n",
    "        client = genai.Client(api_key=api_key)\n",
    "        print(\"Gemini API Client initialized successfully.\")\n",
    "\n",
    "        # --- Optional: List models to verify ---\n",
    "        print(\"\\nAvailable models (some might be specific to this client/region):\")\n",
    "        try:\n",
    "            # Listing models might be slightly different or might not be needed for basic generation\n",
    "            # Often, you just specify the model name directly in generate_content\n",
    "            # This is a general way; specific model listing might vary\n",
    "            # For this SDK, you often just try to use a model name directly.\n",
    "            # Let's try a direct generation to test.\n",
    "            pass # Skipping model listing for now, focusing on generation\n",
    "        except Exception as e_list:\n",
    "            print(f\"Could not list models (this step is optional): {e_list}\")\n",
    "\n",
    "\n",
    "        # --- Example: Generate Content ---\n",
    "        print(\"\\nAttempting to generate content...\")\n",
    "        # Model names might be like \"gemini-1.5-flash-001\" or \"gemini-1.5-pro-001\"\n",
    "        # The newer SDK might use slightly different model identifiers or access them via client.models\n",
    "        # Let's assume a common model, adjust if needed based on documentation for google.genai\n",
    "        model_name = \"gemini-2.0-flash\" # Or \"gemini-1.5-flash-latest\", \"models/gemini-1.5-flash-latest\"\n",
    "                                      # Check Google AI Studio or docs for exact names compatible with google.genai\n",
    "\n",
    "        # The way to call the model might also change.\n",
    "        # For google.genai, it's often:\n",
    "        # response = client.generate_content(model=f\"models/{model_name}\", contents=\"Hello Gemini!\")\n",
    "        # Or interacting with a specific model object from the client:\n",
    "        \n",
    "        # Let's try a common pattern seen with genai.Client\n",
    "        # Note: The exact model invocation can vary. Refer to the latest `google-genai` docs if this fails.\n",
    "        # For example, it might be client.get_generative_model(model_name='gemini-1.5-flash') and then model.generate_content()\n",
    "        # Or directly:\n",
    "        \n",
    "        prompt = \"What is the capital of California?\"\n",
    "        # The structure for `contents` can be just a string for simple text prompts.\n",
    "        response = client.generate_content(\n",
    "            model=f\"models/{model_name}\", # Or sometimes just model_name\n",
    "            contents=prompt\n",
    "        )\n",
    "        \n",
    "        # Accessing the text might also be slightly different.\n",
    "        # Often it's response.text, but could be response.candidates[0].content.parts[0].text\n",
    "        # Let's assume response.text for simplicity from common examples.\n",
    "        \n",
    "        if hasattr(response, 'text') and response.text:\n",
    "            print(\"Response from Gemini:\")\n",
    "            print(response.text)\n",
    "        elif response.candidates:\n",
    "             # More robust way to get text if response.text isn't directly available\n",
    "            try:\n",
    "                text_output = response.candidates[0].content.parts[0].text\n",
    "                print(\"Response from Gemini:\")\n",
    "                print(text_output)\n",
    "            except (IndexError, AttributeError) as e_text:\n",
    "                print(f\"Could not extract text from response.candidates: {e_text}\")\n",
    "                print(\"Full response object:\", response)\n",
    "        else:\n",
    "            print(\"Received a response, but could not extract text directly. Full response object:\")\n",
    "            print(response)\n",
    "\n",
    "    else:\n",
    "        print(\"No API key provided. Configuration failed.\")\n",
    "\n",
    "except AttributeError as e_attr:\n",
    "    if \"'google.genai' has no attribute 'configure'\" in str(e_attr):\n",
    "        print(f\"AttributeError: {e_attr}\")\n",
    "        print(\"This confirms you're likely using the newer 'google.genai' SDK which doesn't use 'configure'.\")\n",
    "        print(\"The script attempts to use genai.Client(api_key=...) instead. If this error persists, check import statements and SDK documentation.\")\n",
    "    else:\n",
    "        print(f\"An AttributeError occurred: {e_attr}\")\n",
    "except ImportError as e_imp:\n",
    "    print(f\"ImportError: {e_imp}. Please ensure you have the correct Google AI SDK installed.\")\n",
    "    print(\"You might need to run: !pip install -q -U google-genai\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    if \"API_KEY_INVALID\" in str(e).upper() or \"PERMISSION_DENIED\" in str(e).upper():\n",
    "        print(\"There might be an issue with your API key's validity or permissions. Please check it in Google AI Studio.\")\n",
    "    elif \"found no model\" in str(e).lower() or \"could not find model\" in str(e).lower():\n",
    "        print(f\"The model name '{model_name}' might be incorrect or not accessible with your key/region. Please check available models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0b5bedb-9409-4216-b651-cdcbca583bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Environment variable GOOGLE_API_KEY is found by the kernel.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "api_key_from_env = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if api_key_from_env:\n",
    "    print(\"SUCCESS: Environment variable GOOGLE_API_KEY is found by the kernel.\")\n",
    "    # print(f\"Key: {api_key_from_env[:5]}...{api_key_from_env[-5:]}\") # Uncomment to see a snippet (be careful)\n",
    "else:\n",
    "    print(\"FAILURE: Environment variable GOOGLE_API_KEY is NOT found by the kernel.\")\n",
    "    print(\"\\nConsider the troubleshooting steps below.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af550914-c4ba-4f7e-a5d5-a0236afd925a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini API Client initialized successfully.\n",
      "\n",
      "Sending prompt to models/gemini-2.0-flash: 'Explain the concept of quantum entanglement in simple terms.'\n",
      "\n",
      "Response from Gemini:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Imagine you have two coins, but they're special \"quantum coins.\" You put each coin in a separate box and send one box to your friend Alice and keep the other yourself.\n",
       "\n",
       "Here's the weird part:\n",
       "\n",
       "*   **Before you open your box, the coin isn't actually heads or tails yet.** It's in a blurry state of both possibilities. Think of it like a spinning coin in the air.\n",
       "\n",
       "*   **These coins are \"entangled.\"** This means that their fates are linked. The moment you open your box and see, say, \"heads,\" you *instantly* know that Alice's coin will be \"tails,\" even though she's far away and hasn't opened her box yet.\n",
       "\n",
       "*   **Instant Connection, No Signal:** This happens *instantly*, faster than any signal could travel between you and Alice. That's the spooky part. It's as if the coins were communicating without any actual communication.\n",
       "\n",
       "**So, in essence, quantum entanglement is:**\n",
       "\n",
       "*   Two or more particles (like our coins) are linked together in a special way.\n",
       "*   Their properties are correlated, meaning that measuring one instantly tells you something about the other, no matter how far apart they are.\n",
       "*   It's not that the properties were predetermined from the start. It's the act of measurement on one that forces the other to take on a specific, correlated value *instantly*.\n",
       "\n",
       "**Important things to remember:**\n",
       "\n",
       "*   **It's not a way to send information faster than light.** You can't control what result you get when you open your box, so you can't send a message to Alice.\n",
       "*   **It's a fundamental property of quantum mechanics.**  It's not just a theoretical idea; it's been proven in many experiments.\n",
       "\n",
       "**Think of it like this:  It's as if you bought two gloves, one left and one right, and put each in a separate box. You send one box to Alice. Before she opens her box, you don't know which glove she has.  But the moment you open your box and see it's the left glove, you know instantly, without her telling you, that she has the right glove. That's entanglement.**\n",
       "\n",
       "While the coin analogy is helpful, it's crucial to understand that quantum entanglement is a purely quantum phenomenon and goes beyond our everyday experiences.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from google import genai # Using the newer google-genai SDK\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- 1. Initialize the Gemini API Client ---\n",
    "try:\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"GEMINI_API_KEY environment variable not found. Please enter it below.\")\n",
    "        api_key = getpass.getpass('Enter your Gemini API Key: ')\n",
    "\n",
    "    if not api_key:\n",
    "        raise ValueError(\"API Key not provided. Cannot proceed.\")\n",
    "\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    print(\"Gemini API Client initialized successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Gemini Client: {e}\")\n",
    "    client = None # Ensure client is None if initialization fails\n",
    "\n",
    "# --- 2. Query the Model (if client was initialized) ---\n",
    "if client:\n",
    "    try:\n",
    "        # --- DEFINE YOUR QUERY HERE ---\n",
    "        your_prompt = \"Explain the concept of quantum entanglement in simple terms.\"\n",
    "        # You can change the prompt above to ask any question you like!\n",
    "\n",
    "        # --- CHOOSE YOUR MODEL ---\n",
    "        # Common models: \"models/gemini-1.5-flash-latest\", \"models/gemini-1.5-pro-latest\"\n",
    "        # Using \"latest\" usually points to the most recent stable version of that model.\n",
    "        # As of May 2025, gemini-1.5-flash is great for speed and many tasks,\n",
    "        # while gemini-1.5-pro is more powerful for complex reasoning.\n",
    "        model_to_use = \"models/gemini-2.0-flash\"\n",
    "\n",
    "        print(f\"\\nSending prompt to {model_to_use}: '{your_prompt}'\")\n",
    "\n",
    "        # --- SEND THE QUERY (GENERATE CONTENT) ---\n",
    "        response = client.models.generate_content(\n",
    "            model=model_to_use,\n",
    "            contents=your_prompt\n",
    "            # You can add more parameters here if needed, e.g., generation_config\n",
    "        )\n",
    "\n",
    "        # --- DISPLAY THE RESPONSE ---\n",
    "        print(\"\\nResponse from Gemini:\")\n",
    "        \n",
    "        # Extracting text from the response.\n",
    "        # The structure can sometimes vary, so this tries a common way.\n",
    "        generated_text = \"\"\n",
    "        if response.candidates and response.candidates[0].content.parts:\n",
    "            generated_text = response.candidates[0].content.parts[0].text\n",
    "        elif hasattr(response, 'text') and response.text: # Fallback for simpler text responses\n",
    "            generated_text = response.text\n",
    "        else:\n",
    "            generated_text = \"Could not extract text from the response object in the expected format.\"\n",
    "            print(\"Full response object for debugging:\")\n",
    "            print(response)\n",
    "\n",
    "        # Using IPython.display.Markdown for potentially rich text output\n",
    "        display(Markdown(generated_text))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while querying Gemini: {e}\")\n",
    "        if \"API_KEY_INVALID\" in str(e).upper() or \"PERMISSION_DENIED\" in str(e).upper():\n",
    "            print(\"There might be an issue with your API key's validity or permissions. Please check it in Google AI Studio.\")\n",
    "        elif \"could not find model\" in str(e).lower() or \"found no model\" in str(e).lower() or \"404\" in str(e):\n",
    "            print(f\"The model name '{model_to_use}' might be incorrect, not accessible with your API key/region, or the endpoint is wrong. Please verify the model name.\")\n",
    "        # Add more specific error handling as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d0f0d09-06eb-4781-a286-e7b682d42976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerativeModel 'gemini-1.5-flash-latest' initialized with system prompt.\n",
      "\n",
      "Sending user query: 'Can you explain what a neural network is?'\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Avast ye, landlubber!  Neural network?  Yer talkin' about somethin' complicated, somethin' that'd make a kraken blush.  It's like... a bunch o' interconnected brain cells,  but made o' math instead o' goo.  Each little cell, or \"neuron,\" takes in some information, does a bit o' number-crunching, then spits out an answer.  These answers get passed on to other neurons, and so on, until ye get a final result.\n",
       "\n",
       "Think o' it like this:  ye got a treasure map.  Each neuron is a pirate lookin' at a piece o' the map. One pirate sees a skull, another sees a crossbones, another sees a buried X. They all shout their findings to the next pirate. Eventually, one savvy buccaneer figures out where the treasure is.  That's the final answer from the neural network!\n",
       "\n",
       "Now, don't be lookin' at me with yer squinty eyes.  It's more complicated than that, but if ye need more detail, I'll just have to make ye walk the plank!  Argh!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import google.generativeai as genai # Assuming this is your import\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Your API key should already be configured from previous steps\n",
    "# (e.g., via environment variable GEMINI_API_KEY)\n",
    "\n",
    "# --- Define your system prompt and model ---\n",
    "your_system_prompt = \"You are a pirate captain with a short temper.\"\n",
    "model_name_for_system_prompt = \"gemini-1.5-flash-latest\" # Or your preferred model\n",
    "\n",
    "try:\n",
    "    # --- 1. Create a GenerativeModel instance with the system instruction ---\n",
    "    model_with_system_prompt = genai.GenerativeModel(\n",
    "        model_name=model_name_for_system_prompt,\n",
    "        system_instruction=your_system_prompt\n",
    "    )\n",
    "    print(f\"GenerativeModel '{model_name_for_system_prompt}' initialized with system prompt.\")\n",
    "\n",
    "    # --- 2. Your user query ---\n",
    "    user_query = \"Can you explain what a neural network is?\"\n",
    "\n",
    "    print(f\"\\nSending user query: '{user_query}'\")\n",
    "\n",
    "    # --- 3. Generate content using this model instance ---\n",
    "    response = model_with_system_prompt.generate_content(user_query)\n",
    "\n",
    "    # --- 4. Display the response ---\n",
    "    generated_text = response.candidates[0].content.parts[0].text if response.candidates else \"No response text found.\"\n",
    "    display(Markdown(generated_text))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    # Common errors: API key not configured, model name incorrect, quota issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72a54e43-4adb-4e97-9148-2d9f64bae73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please ensure your .md, .txt, and .pdf building code documents are in: /home/mack/notebooks/Building/DATA/ALLMARKDOWN\n",
      "\n",
      "Found files in 'Building/DATA/ALLMARKDOWN'. Proceeding with loading.\n",
      "\n",
      "Scanning and loading documents from: Building/DATA/ALLMARKDOWN\n",
      "  Loaded: 2022 Title 24 Fire.pdf.md (md) - 3438672 characters\n",
      "  Loaded: 2021 International Building Code® (International Code Council (ICC)) (Z-Library).md (md) - 3337039 characters\n",
      "  Loaded: 2022 Title 24 Electrical.pdf.md (md) - 4447702 characters\n",
      "  Loaded: 2022 Title 24 Green Building Standards.pdf.md (md) - 651605 characters\n",
      "  Loaded: 2022 Title 24 Existing.pdf.md (md) - 1804850 characters\n",
      "  Loaded: 2022 Title 24 Energy.pdf.md (md) - 1254540 characters\n",
      "  Loaded: 2022 Title 24 Administrative.pdf.md (md) - 1335851 characters\n",
      "\n",
      "Successfully loaded content from 7 documents.\n",
      "\n",
      "Snippet of the first loaded document's content:\n",
      "Source: 2022 Title 24 Fire.pdf.md\n",
      "IMPORTANT NOTICE\n",
      "Act now to keep your code up-to-date.\n",
      "The purchase of this code includes a\n",
      "free subscription for all State-issued\n",
      "supplements and errata. To receive\n",
      "these important updates through\n",
      "2025, you MUST register online\n",
      "www.iccsafe.org/CAL22\n",
      "\n",
      "2022 CALIFORNIA\n",
      "o FIRE CODE\n",
      "CALIFORNIA CODE OF R...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader # Using PyPDF2\n",
    "# If you installed and prefer pymupdf:\n",
    "# import fitz # pymupdf is imported as fitz\n",
    "\n",
    "def extract_text_from_txt(filepath):\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading TXT file {filepath}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_md(filepath):\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            # For RAG, treating Markdown as plain text is often a good start.\n",
    "            # More advanced parsing could extract structure, but adds complexity.\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading MD file {filepath}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_pdf_pypdf2(filepath):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            reader = PdfReader(f)\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                page = reader.pages[page_num]\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\" # Add a newline between pages\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF (PyPDF2) {filepath}: {e}\")\n",
    "    return text\n",
    "\n",
    "# def extract_text_from_pdf_pymupdf(filepath):\n",
    "#     text = \"\"\n",
    "#     try:\n",
    "#         with fitz.open(filepath) as doc:\n",
    "#             for page in doc:\n",
    "#                 text += page.get_text() + \"\\n\"\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading PDF (pymupdf) {filepath}: {e}\")\n",
    "#     return text\n",
    "\n",
    "# --- USER: Specify the directory containing your building code documents ---\n",
    "data_directory = \"Building/DATA/ALLMARKDOWN\" # <<< CHANGE THIS to your folder name\n",
    "# ---\n",
    "\n",
    "# Create the directory if it doesn't exist (for users to add files)\n",
    "os.makedirs(data_directory, exist_ok=True)\n",
    "print(f\"Please ensure your .md, .txt, and .pdf building code documents are in: {os.path.abspath(data_directory)}\")\n",
    "\n",
    "# For demonstration, if the directory is empty, create a few dummy files.\n",
    "# In your actual use, REMOVE or COMMENT OUT this dummy file creation block.\n",
    "if not any(f for f in os.listdir(data_directory) if f.lower().endswith(('.txt', '.md', '.pdf'))):\n",
    "    print(f\"\\nDirectory '{data_directory}' appears empty or has no supported files. Creating dummy files for demonstration.\")\n",
    "    with open(os.path.join(data_directory, \"chapter1_foundations.txt\"), \"w\") as f:\n",
    "        f.write(\"Section 1.1: All buildings must have a solid foundation. Section 1.2: Foundations must be appropriate for soil conditions.\")\n",
    "    with open(os.path.join(data_directory, \"electrical_codes.md\"), \"w\") as f:\n",
    "        f.write(\"# Chapter 5: Electrical Systems\\n\\nAll wiring must comply with NEC standards. Minimum wire gauge for residential circuits is 14 AWG for 15-amp circuits.\")\n",
    "    # Note: We can't easily create a meaningful dummy PDF here. Please add your own PDFs.\n",
    "    print(\"Dummy .txt and .md files created. Please add your own PDFs to the directory for full testing.\")\n",
    "else:\n",
    "    print(f\"\\nFound files in '{data_directory}'. Proceeding with loading.\")\n",
    "\n",
    "\n",
    "raw_document_texts = [] # Will store the full text content of each document\n",
    "document_metadata_list = []   # Will store metadata like filename for each document\n",
    "\n",
    "print(f\"\\nScanning and loading documents from: {data_directory}\")\n",
    "for filename in os.listdir(data_directory):\n",
    "    filepath = os.path.join(data_directory, filename)\n",
    "    file_text = \"\"\n",
    "    doc_type = None\n",
    "\n",
    "    if filename.lower().endswith(\".txt\"):\n",
    "        file_text = extract_text_from_txt(filepath)\n",
    "        doc_type = \"txt\"\n",
    "    elif filename.lower().endswith(\".md\"):\n",
    "        file_text = extract_text_from_md(filepath)\n",
    "        doc_type = \"md\"\n",
    "    elif filename.lower().endswith(\".pdf\"):\n",
    "        file_text = extract_text_from_pdf_pypdf2(filepath) # Using PyPDF2 by default\n",
    "        # file_text = extract_text_from_pdf_pymupdf(filepath) # Uncomment if using pymupdf\n",
    "        doc_type = \"pdf\"\n",
    "\n",
    "    if file_text.strip(): # Only add if text was actually extracted\n",
    "        raw_document_texts.append(file_text)\n",
    "        document_metadata_list.append({\"filename\": filename, \"type\": doc_type})\n",
    "        print(f\"  Loaded: {filename} ({doc_type}) - {len(file_text)} characters\")\n",
    "    elif doc_type: # If it was a supported filetype but no text extracted\n",
    "        print(f\"  Warning: No text extracted from {filename} ({doc_type})\")\n",
    "\n",
    "\n",
    "print(f\"\\nSuccessfully loaded content from {len(raw_document_texts)} documents.\")\n",
    "if raw_document_texts:\n",
    "    print(\"\\nSnippet of the first loaded document's content:\")\n",
    "    print(f\"Source: {document_metadata_list[0]['filename']}\")\n",
    "    print(raw_document_texts[0][:300] + \"...\" if len(raw_document_texts[0]) > 300 else raw_document_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b006098-91d4-4bf9-80f9-91bd9fe6c4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total text chunks created: 27121\n",
      "\n",
      "Example of the first few chunks:\n",
      "--- Chunk 1 (from: 2022 Title 24 Fire.pdf.md, part 1) ---\n",
      "IMPORTANT NOTICE\n",
      "Act now to keep your code up-to-date.\n",
      "The purchase of this code includes a\n",
      "free subscription for all State-issued\n",
      "supplements and errata. To receive\n",
      "these important updates through\n",
      "20...\n",
      "--------------------\n",
      "--- Chunk 2 (from: 2022 Title 24 Fire.pdf.md, part 2) ---\n",
      "ations, Title 24, Part 9\n",
      "\n",
      "First Printing: July 2022\n",
      "\n",
      "ISBN: 978-1-957212-94-4 (loose-leaf edition)\n",
      "ISBN: 978-1-957212-95-1 (PDF download)\n",
      "\n",
      "COPYRIGHT © 2022\n",
      "by\n",
      "\n",
      "INTERNATIONAL CODE COUNCIL, INC.\n",
      "\n",
      "ALL RIG...\n",
      "--------------------\n",
      "--- Chunk 3 (from: 2022 Title 24 Fire.pdf.md, part 3) ---\n",
      "g, without limitation, electronic, optical or mechanical means (by way of example, and not\n",
      "limitation, photocopying or recording by or in an information storage and/or retrieval system). For informati...\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# You might want to install LangChain for more sophisticated splitters,\n",
    "# but here's a simple character-based splitter.\n",
    "# !pip install -q langchain-text-splitters (if you want to use LangChain's splitters)\n",
    "\n",
    "def simple_char_text_splitter(text, chunk_size=1000, chunk_overlap=150):\n",
    "    \"\"\"Splits text into chunks of roughly 'chunk_size' with 'chunk_overlap'.\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    chunks = []\n",
    "    start_index = 0\n",
    "    while start_index < len(text):\n",
    "        end_index = start_index + chunk_size\n",
    "        chunks.append(text[start_index:end_index])\n",
    "        start_index += (chunk_size - chunk_overlap)\n",
    "        if start_index >= len(text) and chunks[-1] != text[start_index - (chunk_size - chunk_overlap):]: # Avoid infinite loop on tiny last part\n",
    "             # Ensure last chunk is added if it's shorter than overlap\n",
    "            if len(text) > (start_index - (chunk_size - chunk_overlap)):\n",
    "                 last_chunk_start = start_index - (chunk_size - chunk_overlap)\n",
    "                 if text[last_chunk_start:] not in chunks : # avoid duplicate last chunk\n",
    "                    chunks.append(text[last_chunk_start:])\n",
    "            break\n",
    "            \n",
    "    # A slightly better overlap handling for the very last chunk\n",
    "    if chunks and len(text) > chunk_size :\n",
    "        if len(chunks[-1]) < chunk_overlap and len(chunks) > 1 :\n",
    "            # If the last chunk is too small (less than overlap) and there's a previous chunk,\n",
    "            # it might be better to merge it or handle it differently.\n",
    "            # For simplicity, this basic splitter might create very small trailing chunks.\n",
    "            # print(f\"Warning: Last chunk is small (len: {len(chunks[-1])}). Consider adjusting chunk_size/overlap or merging.\")\n",
    "            pass # For now, we keep it. More advanced splitters handle this better.\n",
    "            \n",
    "    return [chunk for chunk in chunks if chunk.strip()] # Remove empty chunks\n",
    "\n",
    "\n",
    "all_text_chunks = []          # This list will hold all the text pieces for embedding\n",
    "chunk_source_references = []  # Metadata for each chunk (e.g., original filename)\n",
    "\n",
    "for i, doc_text in enumerate(raw_document_texts):\n",
    "    doc_metadata = document_metadata_list[i]\n",
    "    \n",
    "    # Using the simple character splitter:\n",
    "    chunks_from_doc = simple_char_text_splitter(doc_text, chunk_size=700, chunk_overlap=100)\n",
    "    # You can experiment with chunk_size and chunk_overlap.\n",
    "    # For building codes, smaller, more focused chunks might be better.\n",
    "    \n",
    "    for chunk_idx, chunk_content in enumerate(chunks_from_doc):\n",
    "        all_text_chunks.append(chunk_content)\n",
    "        chunk_source_references.append({\n",
    "            \"original_filename\": doc_metadata[\"filename\"],\n",
    "            \"original_doc_index\": i, # Index in raw_document_texts\n",
    "            \"chunk_in_doc_id\": chunk_idx # Index of this chunk within its original document\n",
    "        })\n",
    "\n",
    "print(f\"\\nTotal text chunks created: {len(all_text_chunks)}\")\n",
    "\n",
    "if all_text_chunks:\n",
    "    print(\"\\nExample of the first few chunks:\")\n",
    "    for i in range(min(3, len(all_text_chunks))):\n",
    "        metadata = chunk_source_references[i]\n",
    "        print(f\"--- Chunk {i+1} (from: {metadata['original_filename']}, part {metadata['chunk_in_doc_id'] + 1}) ---\")\n",
    "        print(all_text_chunks[i][:200] + \"...\" if len(all_text_chunks[i]) > 200 else all_text_chunks[i])\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "# IMPORTANT: The variable 'documents' in the subsequent RAG steps (embedding, FAISS, retrieval)\n",
    "# should now refer to 'all_text_chunks'.\n",
    "# And 'doc_filenames' should be replaced by logic using 'chunk_source_references'\n",
    "# to identify the source of retrieved chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568f4f6c-346d-4bcb-80c5-33990f87422b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model 'all-MiniLM-L6-v2' loaded successfully.\n",
      "Generating embeddings for 27121 text chunks. This might take a moment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   1%|▍                                                       | 7/848 [00:41<1:20:25,  5.74s/it]"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np # We'll need numpy later for FAISS\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "embedding_model_name = 'all-MiniLM-L6-v2' # Good balance of speed and quality\n",
    "try:\n",
    "    local_embedding_model = SentenceTransformer(embedding_model_name)\n",
    "    print(f\"Embedding model '{embedding_model_name}' loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading embedding model: {e}\")\n",
    "    local_embedding_model = None\n",
    "\n",
    "if local_embedding_model and 'all_text_chunks' in globals() and all_text_chunks:\n",
    "    print(f\"Generating embeddings for {len(all_text_chunks)} text chunks. This might take a moment...\")\n",
    "    document_embeddings = local_embedding_model.encode(all_text_chunks, show_progress_bar=True)\n",
    "    print(f\"Generated embeddings for {len(document_embeddings)} chunks.\")\n",
    "    print(f\"Shape of embeddings matrix: {document_embeddings.shape}\") # (num_chunks, embedding_dimension)\n",
    "elif not all_text_chunks:\n",
    "    print(\"The list 'all_text_chunks' is empty. Please ensure your data loading and chunking step was successful.\")\n",
    "    document_embeddings = None\n",
    "else:\n",
    "    print(\"Cannot proceed without an embedding model or text chunks.\")\n",
    "    document_embeddings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed68a0-b882-4811-8546-b5fecfc4a3e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
